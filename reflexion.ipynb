{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Partie 3 : R√©flexion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imaginons que Julie, une data analyst de l‚Äô√©quipe marketing, souhaite acc√©der aux informations des utilisateurs et aux actions qu‚Äôils effectuent sur le site (consultations de pages, clics, etc.) afin d‚Äôam√©liorer les analyses marketing. Ces donn√©es sont disponibles via une API, mais elles doivent √™tre mises √† disposition de mani√®re structur√©e et accessible pour Julie.\n",
    "\n",
    "üéØ Votre mission\n",
    "En tant que data engineer Meilleurtaux, votre r√¥le est de r√©pondre √† cette demande en mettant en place un processus pour r√©cup√©rer et stocker les donn√©es de l‚ÄôAPI, afin que Julie puisse les utiliser facilement dans ses analyses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposition de solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- üîó `Acc√®s aux donn√©es : Comment allez-vous r√©cup√©rer les donn√©es de l‚ÄôAPI ?` <br>\n",
    "Pour ce qui de la partie acces aux donn√©es, il faut mettre en place un outil d'ingestion des donn√©es de l'API. Il y'a plusieurs facon de faire cela. On peut choisir une approche tres simple en utilis√© un outil d'ingestion comme Airbyte ou on a pas besoin d'√©crire du code pour d√©placer nos donn√©es de la source vers la destination. Mais ici, je propose d'√©crire un script python qui va faire un appel API et load les donn√©es dans notre data warehouse. On pourra dockeris√© cette application python et utili√© cloud Run par exemple pour l'executer. <br>\n",
    "- üíæ `Stockage : O√π allez-vous stocker les donn√©es pour qu‚Äôelles soient facilement accessibles pour Julie et l‚Äô√©quipe marketing ?`<br>\n",
    "Pour la partie stockage, on peut choisir un data warehouse cloud based comme BigQuery ou encore Snowflake. Il s'agit des deux warehouse les plus utilis√©s dans la modern data stack aujourd'hui. L'avantage est qu'on pourra mettre en place une architecture medallion afin d'avoir une meilleure organisation de la donn√©es. En plus de cela, les outils de BI utilis√©s par les Data Analyst se plugent facilement √° BigQuery ou Snowflake sans probleme. <br>\n",
    "- üîÑ `Automatisation : Comment mettre en place un processus automatis√© (par exemple, toutes les heures ou chaque jour) pour garantir que les donn√©es sont toujours √† jour ?` <br>\n",
    "Pour la partie automatisation, il faudra faire appel √° un orchestrateur de workflows qui pourra executer le data pipeline de facon automatis√© en fonction des besoins m√©tier. On a plusieurs choix dans ce sens : Airflow ou Dadster. En consid√©rant le fait qu'on utilise BigQuery, on peut utiliser Cloud composer qui est la version GCP de airflow pour orchestrer les transformation qu'on va effectuer mais aussi l'extration depuis l'API. <br>\n",
    "- üõ†Ô∏è `D√©ploiement et Outils : Comment allez-vous d√©ployer votre solution, et quels outils ou technologies utiliseriez-vous ?`<br>\n",
    "Notre solution sera cloud based. Donc il eest tres facile √° d√©ployer. On pourra faire recours √©videmment √° de l'IaC pour g√©n√©rer l'infrastructure, automatiser le d√©ploiement de notre script qui fetch les donn√©es depuis l'API avec github actions par exemple.\n",
    "La liste d'outils propos√©es est la suivante :\n",
    "     * Airbyte ou Dockeris√© un script python : pour la partie ingestion des donn√©es\n",
    "     * BigQuery : Data Warehouse pour le stockage / snowflake aussi est une option\n",
    "     * DBT : S'il y'a le besoin de faire des transformation afin utilisation\n",
    "     * Airflow : pour orchestrer et automatiser le workflow \n",
    "     * Github Action : automatiser le deploiement du script au cas ou on utilise pas airbyte pour l'ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
